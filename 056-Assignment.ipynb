{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the Environment\n",
    "First, you need to import the libraries you'll need. You can also import them as you find them necessary.\n",
    "\n",
    "Task 5.7.1: Import the libraries you'll need. You can update this cell and re-run it as you discover more things later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries that you need\n",
    "from pathlib import Path\n",
    "\n",
    "import medigan\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm.notebook import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since GPUs are available on your machine, make sure you handle placing the tensors on the proper device.\n",
    "\n",
    "Task 5.7.2: Check the availability of GPUs on this machine and determine the correct device name. Store the device name in the variable device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using {device} device.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting Images\n",
    "We don't have a nice collection of images to work with. Instead, we'll need to have Medigan generate them for us. Let's find one that does what we need.\n",
    "\n",
    "Task 5.7.3: Find a GAN in Medigan that produces mammogram images with the roi (\"region of interest\") marked, and save its id to model_id. You'll need to create the entry-point for Medigan as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00004_PIX2PIX_MMG_MASSES_W_MASKS\n"
     ]
    }
   ],
   "source": [
    "# Create the connection to the Medigan generators\n",
    "generators = medigan.Generators()\n",
    "\n",
    "# Find the models that match what we want\n",
    "values = [\"mammogram\", \"roi\"]\n",
    "models = generators.find_matching_models_by_values(values)\n",
    "model_id = models[0].model_id\n",
    "\n",
    "print(model_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have only gotten one model back. Let's make sure this is doing what we want.\n",
    "\n",
    "Task 5.7.4: Get the configuration for the model you found using the model_id. Save the model configuration to the model_config variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model keys: dict_keys(['execution', 'selection', 'description'])\n"
     ]
    }
   ],
   "source": [
    "model_config = generators.get_config_by_id(model_id=model_id)\n",
    "\n",
    "print(f\"Model keys: {model_config.keys()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw before, the configurations have a lot of information in them. Let's only look at the parts we're interested in.\n",
    "\n",
    "Task 5.7.5: Select the generates, tags, height, and width keys from the selection key in the model configuration. Save the result to model_info, a dictionary that maps the selected keys to their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generates': ['regions of interest',\n",
       "  'ROI',\n",
       "  'mammograms',\n",
       "  'patches',\n",
       "  'full-field digital mammograms'],\n",
       " 'tags': ['Mammogram',\n",
       "  'Mammography',\n",
       "  'Digital Mammography',\n",
       "  'Full field Mammography',\n",
       "  'Full-field Mammography',\n",
       "  'pix2pix',\n",
       "  'Pix2Pix',\n",
       "  'Mass segmentation',\n",
       "  'Breast lesion'],\n",
       " 'height': 256,\n",
       " 'width': 256}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals = [\"generates\", \"tags\", \"height\", \"width\"]\n",
    "model_info = {x: model_config[\"selection\"][x] for x in vals}\n",
    "\n",
    "model_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to generate the data. We'll need to define places where we'll store this data.\n",
    "\n",
    "Task 5.7.6: Create a path to the directory output/sample_mammogram. Use Pathlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output\\sample_mammogram\n"
     ]
    }
   ],
   "source": [
    "output_dir = Path(\"output\")\n",
    "sample_dir = output_dir / \"sample_mammogram\"\n",
    "\n",
    "# Create the directory with mkdir\n",
    "sample_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(sample_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If something goes wrong, you can delete the whole directory and start over:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!rm -Rf output/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do mammogram images look like? Let's generate some.\n",
    "Task 5.7.7: Generate 4 images of mammograms with region of interest using our selected GAN. Save them to sample_dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]ERROR:root:Error while trying to initialize pix2pix: No module named ':'\n",
      "ERROR:root:Error while trying to generate 4 images with model models/00004_PIX2PIX_MMG_MASSES_W_MASKS/pix2pix_mask_to_mass_model.pth: No module named ':'\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR:root:00004_PIX2PIX_MMG_MASSES_W_MASKS: Error while trying to generate images with model models/00004_PIX2PIX_MMG_MASSES_W_MASKS/pix2pix_mask_to_mass_model.pth: No module named ':'\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named ':'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgenerators\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_dir\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\medigan\\generators.py:796\u001b[0m, in \u001b[0;36mGenerators.generate\u001b[1;34m(self, model_id, num_samples, output_path, save_images, is_gen_function_returned, install_dependencies, **kwargs)\u001b[0m\n\u001b[0;32m    791\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_manager\u001b[38;5;241m.\u001b[39mmatch_model_id(provided_model_id\u001b[38;5;241m=\u001b[39mmodel_id)\n\u001b[0;32m    793\u001b[0m model_executor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model_executor(\n\u001b[0;32m    794\u001b[0m     model_id\u001b[38;5;241m=\u001b[39mmodel_id, install_dependencies\u001b[38;5;241m=\u001b[39minstall_dependencies\n\u001b[0;32m    795\u001b[0m )\n\u001b[1;32m--> 796\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_images\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_gen_function_returned\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_gen_function_returned\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\medigan\\execute_model\\model_executor.py:391\u001b[0m, in \u001b[0;36mModelExecutor.generate\u001b[1;34m(self, num_samples, output_path, save_images, is_gen_function_returned, batch_size, **kwargs)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    387\u001b[0m     logging\u001b[38;5;241m.\u001b[39merror(\n\u001b[0;32m    388\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Error while trying to generate images with model \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserialised_model_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    390\u001b[0m     )\n\u001b[1;32m--> 391\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\medigan\\execute_model\\model_executor.py:371\u001b[0m, in \u001b[0;36mModelExecutor.generate\u001b[1;34m(self, num_samples, output_path, save_images, is_gen_function_returned, batch_size, **kwargs)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m Utils\u001b[38;5;241m.\u001b[39mmkdirs(\n\u001b[0;32m    366\u001b[0m     path_as_string\u001b[38;5;241m=\u001b[39mbatch_path\n\u001b[0;32m    367\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: The batch path was not found nor created in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    369\u001b[0m prepared_kwargs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch_path})\n\u001b[1;32m--> 371\u001b[0m \u001b[43mgenerate_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepared_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(batch_path):\n\u001b[0;32m    374\u001b[0m     os\u001b[38;5;241m.\u001b[39mrename(\n\u001b[0;32m    375\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(batch_path, filename),\n\u001b[0;32m    376\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m    377\u001b[0m             output_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(batch_num) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m filename\n\u001b[0;32m    378\u001b[0m         ),\n\u001b[0;32m    379\u001b[0m     )\n",
      "File \u001b[1;32md:\\WQUniversity\\AI LAB\\5. Medical Data in Spain\\models\\00004_PIX2PIX_MMG_MASSES_W_MASKS\\__init__.py:164\u001b[0m, in \u001b[0;36mgenerate_GAN_images\u001b[1;34m(model_file, image_size, input_path, num_samples, save_images, output_path, shapes, patch_size, ssim_threshold, gpu_id)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    163\u001b[0m     logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while trying to generate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m images with model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32md:\\WQUniversity\\AI LAB\\5. Medical Data in Spain\\models\\00004_PIX2PIX_MMG_MASSES_W_MASKS\\__init__.py:123\u001b[0m, in \u001b[0;36mgenerate_GAN_images\u001b[1;34m(model_file, image_size, input_path, num_samples, save_images, output_path, shapes, patch_size, ssim_threshold, gpu_id)\u001b[0m\n\u001b[0;32m    115\u001b[0m rescale_width \u001b[38;5;241m=\u001b[39m image_size[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    117\u001b[0m logging\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrescale_height: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrescale_height\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoints_dir:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoints_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint_name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu_ids: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgpu_ids\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 123\u001b[0m opt, model \u001b[38;5;241m=\u001b[39m \u001b[43minit_pix2pix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrescale_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoints_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m logging\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating images...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# Rescaling of the image inside the method\u001b[39;00m\n",
      "File \u001b[1;32md:\\WQUniversity\\AI LAB\\5. Medical Data in Spain\\models\\00004_PIX2PIX_MMG_MASSES_W_MASKS\\__init__.py:60\u001b[0m, in \u001b[0;36minit_pix2pix\u001b[1;34m(image_height, checkpoints_dir, checkpoint_name, gpu_ids)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     59\u001b[0m     logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while trying to initialize pix2pix: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32md:\\WQUniversity\\AI LAB\\5. Medical Data in Spain\\models\\00004_PIX2PIX_MMG_MASSES_W_MASKS\\__init__.py:54\u001b[0m, in \u001b[0;36minit_pix2pix\u001b[1;34m(image_height, checkpoints_dir, checkpoint_name, gpu_ids)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     opt \u001b[38;5;241m=\u001b[39m TestOptions(image_height, checkpoints_dir, checkpoint_name, gpu_ids)\n\u001b[1;32m---> 54\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# create a model given opt.model and other options\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     model\u001b[38;5;241m.\u001b[39msetup(opt)  \u001b[38;5;66;03m# regular setup: load and print networks; create schedulers\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32md:\\WQUniversity\\AI LAB\\5. Medical Data in Spain\\models\\00004_PIX2PIX_MMG_MASSES_W_MASKS\\src\\models\\__init__.py:69\u001b[0m, in \u001b[0;36mcreate_model\u001b[1;34m(opt)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_model\u001b[39m(opt):\n\u001b[0;32m     60\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a model given the option.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m    This function warps the class CustomDatasetDataLoader.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m        >>> model = create_model(opt)\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mfind_model_using_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     instance \u001b[38;5;241m=\u001b[39m model(opt)\n\u001b[0;32m     71\u001b[0m     logging\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel [\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m] was created\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(instance)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[1;32md:\\WQUniversity\\AI LAB\\5. Medical Data in Spain\\models\\00004_PIX2PIX_MMG_MASSES_W_MASKS\\src\\models\\__init__.py:38\u001b[0m, in \u001b[0;36mfind_model_using_name\u001b[1;34m(model_name)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m#model_filename = \".src.models.\" + model_name + \"_model\"\u001b[39;00m\n\u001b[0;32m     37\u001b[0m model_filename \u001b[38;5;241m=\u001b[39m models_folder[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m model_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 38\u001b[0m modellib \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     40\u001b[0m target_model_name \u001b[38;5;241m=\u001b[39m model_name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1206\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1178\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1128\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1206\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1178\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1128\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "    \u001b[1;31m[... skipping similar frames: _find_and_load at line 1178 (7 times), _gcd_import at line 1206 (7 times), _call_with_frames_removed at line 241 (6 times), _find_and_load_unlocked at line 1128 (6 times)]\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1128\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1206\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1178\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1142\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named ':'"
     ]
    }
   ],
   "source": [
    "generators.generate(\n",
    "    model_id=model_id,\n",
    "    num_samples=4,\n",
    "    output_path=sample_dir\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5.7.8: Finish up the missing parts of the function view_images. Invoke the function and store the resulting image in the variable sample_images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_images(directory, num_images=4, glob_rule=\"*.jpg\"):\n",
    "    \"\"\"Displays a sample of images in the given directory\n",
    "    They'll display in rows of 4 images\n",
    "    - directory: which directory to look for images\n",
    "    - num_images: how many images to display (default 4, for one row)\n",
    "    - glob_rule: argument to glob to filter images (default \"*\" selects all)\"\"\"\n",
    "\n",
    "    image_list = list(directory.glob(glob_rule))  # REMOVERHS\n",
    "    num_samples = min(num_images, len(image_list))\n",
    "    images = [read_image(str(f)) for f in sorted(image_list)[:num_samples]]  # REMOVERHS\n",
    "    grid = make_grid(images, nrow=4, pad_value=255.0)\n",
    "    return torchvision.transforms.ToPILImage()(grid)\n",
    "\n",
    "\n",
    "sample_images = view_images(sample_dir)\n",
    "sample_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're going to use these generated images to train our model, we'll need a data loader. Medigan can provide one for us directly. We'll make both a training set and a validation set.\n",
    "\n",
    "Task 5.7.9: Use Medigan to create a data loader of training data. Make 50 images, in batches of 4, with shuffling turned on. Don't forget to set prefetch_factor=None. This may take a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = generators.get_as_torch_dataloader(\n",
    "    model_id=model_id, num_samples=50, batch_size=4, shuffle=True, prefetch_factor=None\n",
    ")\n",
    "\n",
    "sample_batch = next(iter(train_dataloader))\n",
    "print(f\"Training data loader with keys: {sample_batch.keys()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5.7.10: Use Medigan to create a data loader of validation data. This time only create \n",
    " images and don't shuffle. All other settings should be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = generators.get_as_torch_dataloader(\n",
    "    model_id=model_id, num_samples=30, batch_size=4, shuffle=False, prefetch_factor=None\n",
    ")\n",
    "\n",
    "val_batch = next(iter(train_dataloader))\n",
    "shape = val_batch[\"sample\"].shape\n",
    "dtype = val_batch[\"sample\"].dtype\n",
    "print(f\"Validation image with data shape {shape} and type {dtype}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data isn't quite what we need for PyTorch. This is particularly apparent with the mask.\n",
    "\n",
    "Task 5.7.11: Get the shape and type for the mask component of the val_batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = val_batch[\"mask\"].shape\n",
    "dtype = val_batch[\"mask\"].dtype\n",
    "\n",
    "print(f\"Validation mask with data shape {shape} and type {dtype}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to fix this, we need the images to be [3, 256, 256] and the mask to be [1, 256, 256], and both to have type float32. This function converts the type and adds the channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_torch_image(tensor, color=False):\n",
    "    tensor_float = tensor.type(torch.float32)\n",
    "    grayscale = tensor_float.unsqueeze(1)\n",
    "    if color:\n",
    "        return grayscale.repeat(1, 3, 1, 1)\n",
    "    else:\n",
    "        return grayscale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5.7.12: Run this function on the mask component of val_batch and get the new shape and type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_converted = convert_to_torch_image(val_batch[\"mask\"])\n",
    "\n",
    "shape = mask_converted.shape\n",
    "dtype = mask_converted.dtype\n",
    "\n",
    "print(f\"Validation mask with data shape {shape} and type {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5.7.13: Run this function on the sample component of val_batch and get the new shape and type. You'll need to specify color=True to get RGB images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_converted = convert_to_torch_image(val_batch[\"sample\"], color=True)\n",
    "\n",
    "shape = sample_converted.shape\n",
    "dtype = sample_converted.dtype\n",
    "\n",
    "print(f\"Validation mask with data shape {shape} and type {dtype}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Model\n",
    "Now that we have our data, we'll want to train a model. This is a segmentation problem, and we found a good pre-trained model for that in one of the lessons. Let's use that one.\n",
    "\n",
    "Task 5.7.14: Load the pre-trained deeplabv3_resnet50 model. Use the COCO_WITH_VOC_LABELS_V1 weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = (\n",
    "    torchvision.models.segmentation.DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1\n",
    ")\n",
    "\n",
    "model = torchvision.models.segmentation.deeplabv3_resnet50(weights=pretrained_weights)\n",
    "\n",
    "print(\"Model components:\")\n",
    "for name, part in model.named_children():\n",
    "    print(\"\\t\" + name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we'll need to replace the final layer with one that does what we need. But first we should see what we get from the model. This model gives a dictionary with two outputs: out and aux. We only want out\n",
    "\n",
    "Task 5.7.15: Run the model on the sample part of our sample_batch, and get the shape of the out part of the result. You'll need to convert the data to the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_converted = convert_to_torch_image(sample_batch[\"sample\"], True)\n",
    "model_result = model(sample_converted)\n",
    "model_out = model_result[\"out\"]\n",
    "out_shape = model_out.shape\n",
    "\n",
    "out_shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't match our masks. It's the right height and width, but the wrong number of channels. We'll replace that last layer. It's a convolution, but not the one we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5.7.16: Replace the last layer in the classifier with a convolution that gives the correct output shape to match our mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_final_layer = torch.nn.Conv2d(256, 1, kernel_size=(1, 1))\n",
    "model.classifier[-1] = new_final_layer\n",
    "\n",
    "new_out = model(sample_converted)[\"out\"]\n",
    "print(f\"New model output shape: {new_out.shape}\")\n",
    "print(f\"Mask shape: {mask_converted.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're also going to need a loss function and an optimizer for when we train. We'll use the same BCEWithLogitsLoss we used in the lesson, and an Adam optimizer.\n",
    "Task 5.7.17: Create the loss function and the optimizer. Save them to loss_fun and opt respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fun = torch.nn.BCEWithLogitsLoss()\n",
    "opt = torch.optim.Adam(params=model.parameters())\n",
    "\n",
    "\n",
    "opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "We'll need to train the model we just created. It'll be quite similar to training we've done in the past, but slightly adjusted due to the need to adjust the image and mask shapes, and getting the out part of the output.\n",
    "\n",
    "We'll build up a few functions to put this together. First, we'll deal with calculating the loss. The function below outlines this, but the details are missing.\n",
    "\n",
    "Task 5.7.18: Fill in the missing parts of the compute_loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(batch, model, loss_fun):\n",
    "    # Extract the sample and mask from the batch\n",
    "    sample = batch[\"sample\"]\n",
    "    mask = batch[\"mask\"]\n",
    "\n",
    "    # Convert the sample and mask to the correct shape and type\n",
    "    sample_correct = convert_to_torch_image(sample, color=True)\n",
    "    mask_correct = convert_to_torch_image(mask)\n",
    "\n",
    "    # move the sample and mask to the GPU\n",
    "    sample_gpu = sample_correct.to(device)\n",
    "    mask_gpu = mask_correct.to(device)\n",
    "\n",
    "    # Run the model on the sample and select the classifier (out key)\n",
    "    output = model(sample_gpu)[\"out\"]\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = loss_fun(output, mask_gpu)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll run this on the sample batch to make sure it's working. Note we need the model on the GPU as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "\n",
    "compute_loss(sample_batch, model, loss_fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that working, we can build the training for one epoch. We'll loop over the data loader and step our model for each batch. We'll also compute the validation loss.\n",
    "\n",
    "Task 5.7.19: Fill in the missing parts of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_dataloader, val_dataloader, loss_fun, opt):\n",
    "    model.train()\n",
    "\n",
    "    # Training part\n",
    "    train_loss = 0.0\n",
    "    train_count = 0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # zero the gradients on the optimizer\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # compute the loss for the batch\n",
    "        loss = compute_loss(batch, model, loss_fun)\n",
    "\n",
    "        # Compute the backward part of the loss and step the optimizer\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_count += 1\n",
    "\n",
    "    # Validation part\n",
    "    val_loss = 0.0\n",
    "    val_count = 0\n",
    "    for batch in tqdm(val_dataloader):\n",
    "        # compute the loss for each batch\n",
    "        loss = compute_loss(batch, model, loss_fun)\n",
    "\n",
    "        val_loss += loss.item()\n",
    "        val_count += 1\n",
    "\n",
    "    return train_loss / train_count, val_loss / val_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check this worked by running one epoch. It'll return the two losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch(model, train_dataloader, val_dataloader, loss_fun, opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to go. We can call this in a loop to train our model.\n",
    "Task 5.7.20: Load the pretrained model.\n",
    "\n",
    "We have trained the model for 9 more epochs. Now it's your time, load the model that we have saved in the file model_trained.pth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('model_trained.pth').to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Model\n",
    "Let's see how well we did. First, we'll need some new data to test on.\n",
    "\n",
    "Task 5.7.21: Use Medigan to create a data loader of test data. This time only create \n",
    " images and don't shuffle. All other settings should be the same as our earlier loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = generators.get_as_torch_dataloader(\n",
    "    model_id=model_id, num_samples=8, batch_size=4, shuffle=False, prefetch_factor=None\n",
    ")\n",
    "\n",
    "test_batch = next(iter(test_dataloader))\n",
    "\n",
    "print(f\"Data loader images in batches of {test_batch['sample'].size(0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5.7.22: Run the model on the test_batch, and save the out to test_result. You'll need to first convert the sample to the correct shape and type, and move it to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_sample = convert_to_torch_image(test_batch[\"sample\"], color=True)\n",
    "corrected_sample = corrected_sample.to(device)\n",
    "\n",
    "test_result = model(corrected_sample)[\"out\"]\n",
    "\n",
    "test_result.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing, our result was never put through an activation function. We need it to be image-like, which we can get by applying the sigmoid.\n",
    "\n",
    "Task 5.7.23: Apply the sigmoid function to the test_result. Save the output to test_mask_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mask_model = torch.sigmoid(test_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can plot our results and see how we did. This function expects tensors that are already in the right shape [batch_size, channels, height, width].\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images_from_tensor(tensor):\n",
    "    grid = make_grid(tensor, nrow=4, pad_value=1.0)\n",
    "    return torchvision.transforms.ToPILImage()(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5.7.24: Call the plot function to look at the input sample, the mask, and our final result. How did we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the sample part of the test_batch\n",
    "sample_test_batch_plot = plot_images_from_tensor(convert_to_torch_image(test_batch[\"sample\"]))\n",
    "sample_test_batch_plot\n",
    "\n",
    "# Plot the mask part of the test_batch\n",
    "mask_test_batch_plot = plot_images_from_tensor(convert_to_torch_image(test_batch[\"mask\"]))\n",
    "mask_test_batch_plot\n",
    "\n",
    "# Plot the result of the model running\n",
    "model_result_plot = plot_images_from_tensor(test_mask_model)\n",
    "model_result_plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did your model do? You can try to train longer to see if you get better results, or adjust the learning rate in Adam, or pull a new test batch to see a broader range of results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
